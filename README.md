# llm-notebooks


## Blogs
* [Huggingface Blog about efficient training on a single GPU](https://huggingface.co/docs/transformers/v4.20.1/en/perf_train_gpu_one)
* [Padding large-language-models](https://medium.com/towards-data-science/padding-large-language-models-examples-with-llama-2-199fb10df8ff)
* [How to generate text: using different decoding methods for language generation with Transformers](https://huggingface.co/blog/how-to-generate)
* [Llama.cpp Tutorial](https://www.datacamp.com/tutorial/llama-cpp-tutorial)
* [Introduction to Llama](https://www.datacamp.com/blog/introduction-to-meta-ai-llama)
* [Getting Started with Axolotl for Fine-Tuning LLMs](https://drchrislevy.github.io/posts/intro_fine_tune/intro_fine_tune.html)


## Notebooks and implementations
* [Minimalistic implementation of LoRA with guidelines](https://colab.research.google.com/drive/1QG1ONI3PfxCO2Zcs8eiZmsDbWPl4SftZ)
* [Maxime Labonne's Fine-tune Llama 2 on Google Colab.ipynb](https://colab.research.google.com/drive/1p68M5E5fZ7kSa7nA-e-20489nuFSXVp2?usp=sharing)


## Video Lectures
- [Fine-Tune Llama2 | Step by Step Guide to Customizing Your Own LLM](https://www.youtube.com/watch?v=Pb_RGAl75VE): Great Short introduction on custom SFT data creation and SFT training using TRL


## Leaderboards
- [Massive Text Embedding Benchmark (MTEB) Leaderboard](https://huggingface.co/spaces/mteb/leaderboard)


## LLM Course
- https://github.com/mlabonne/llm-course?tab=readme-ov-file