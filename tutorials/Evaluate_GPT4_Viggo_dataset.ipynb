{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Evaluation of GPT-4o for Functional Representation Extraction\n",
    "\n",
    "- In this notebook, I use OpenAI's [GPT-4o](https://openai.com/index/hello-gpt-4o/) model for structured functional representation extraction. \n",
    "- The goal is to evaluate GPT-4o's performance in extracting structural functional representations from the [VIGGO dataset](https://huggingface.co/datasets/GEM/viggo). \n",
    "- This evaluation will serve as the baseline performance for comparison with other models such as `Claude`, `Gemini`, `Llama-3-70b`, and most importantly, with custom fine-tuned open-source models like `llama-3-8b` and `mistral-7b`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load required libraries\n",
    "from datasets import load_dataset\n",
    "import os, re\n",
    "import random\n",
    "from openai import OpenAI\n",
    "import instructor\n",
    "from pydantic import BaseModel, Field\n",
    "import json\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viggo Dataset\n",
    "- [VIGGO dataset](https://huggingface.co/datasets/GEM/viggo) is a video game opinions data-to-text generation dataset. \n",
    "- It is intended to generate coherent conversational responses based on input functional representations (set of attributes and values). \n",
    "- **In this notebook, I use the reverse task, where we generate structured functional representations from the given text.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset\n",
    "- Since, I want to evaluate `GPT-4o` for structured representation extraction, I will only consider the VIGGO `validation dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['gem_id', 'meaning_representation', 'target', 'references'],\n",
       "        num_rows: 5103\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['gem_id', 'meaning_representation', 'target', 'references'],\n",
       "        num_rows: 714\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['gem_id', 'meaning_representation', 'target', 'references'],\n",
       "        num_rows: 1083\n",
       "    })\n",
       "    challenge_train_1_percent: Dataset({\n",
       "        features: ['gem_id', 'meaning_representation', 'target', 'references'],\n",
       "        num_rows: 50\n",
       "    })\n",
       "    challenge_train_2_percent: Dataset({\n",
       "        features: ['gem_id', 'meaning_representation', 'target', 'references'],\n",
       "        num_rows: 103\n",
       "    })\n",
       "    challenge_train_5_percent: Dataset({\n",
       "        features: ['gem_id', 'meaning_representation', 'target', 'references'],\n",
       "        num_rows: 256\n",
       "    })\n",
       "    challenge_train_10_percent: Dataset({\n",
       "        features: ['gem_id', 'meaning_representation', 'target', 'references'],\n",
       "        num_rows: 510\n",
       "    })\n",
       "    challenge_train_20_percent: Dataset({\n",
       "        features: ['gem_id', 'meaning_representation', 'target', 'references'],\n",
       "        num_rows: 1021\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 444,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the dataset\n",
    "dataset = load_dataset(\"GEM/viggo\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the validation dataset\n",
    "val_dataset = dataset[\"validation\"]\n",
    "\n",
    "# rename \"meaning_representation\" to \"attributes\" and \"target\" to \"text\" for consistency\n",
    "val_dataset = val_dataset.rename_columns({\"meaning_representation\": \"attributes\", \"target\": \"text\"})\n",
    "\n",
    "# remove \"gem_id\" and \"reference\" columns\n",
    "val_dataset = val_dataset.remove_columns([\"gem_id\", \"references\"])\n",
    "\n",
    "# delete the original full dataset\n",
    "del dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A few data examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attributes': \"inform(name[Mirror's Edge Catalyst], genres[action-adventure, fighting], player_perspective[first person], has_multiplayer[yes], platforms[PlayStation, Xbox, PC], available_on_steam[no], has_linux_release[yes], has_mac_release[no])\", 'text': \"Mirror's Edge Catalyst is a first person action-adventure and fighting game. It has multiplayer and released on PlayStation, Xbox, and PC. It has a Linux release but it doesn't have a Mac release and isn't available on Steam.\"}\n",
      "{'attributes': 'inform(name[Grand Theft Auto V], release_year[2013], developer[Rockstar North], genres[action-adventure, driving/racing, shooter], player_perspective[first person, third person])', 'text': \"If you'd like to play an action-adventure game, Grand Theft Auto V is a first and third person game involving driving and shooting. It was developed by Rockstar North and released in 2013.\"}\n",
      "{'attributes': 'inform(name[Horizon: Zero Dawn], esrb[T (for Teen)], genres[action-adventure, role-playing, shooter], platforms[PlayStation])', 'text': 'Horizon: Zero Dawn was released for PlayStation only and is rated T (for Teen). This game is an action-adventure shooter RPG.'}\n",
      "{'attributes': 'verify_attribute(name[Crysis], developer[Crytek Frankfurt], rating[good], player_perspective[first person])', 'text': 'You said that you liked Crysis. Do you often play first person games from Crytek Frankfurt?'}\n",
      "{'attributes': 'request(specifier[fascinating])', 'text': \"Is there any game you've played lately that you felt was fascinating?\"}\n"
     ]
    }
   ],
   "source": [
    "# get 5 random examples from the validation dataset\n",
    "random_seed = 42\n",
    "random_idx_list = random.sample(range(len(val_dataset)), 5)\n",
    "for idx in random_idx_list: print(val_dataset[idx])\n",
    "\n",
    "random_seed = 42\n",
    "num_examples = 5\n",
    "random_idx_list = random.sample(range(len(val_dataset)), num_examples)\n",
    "\n",
    "for i,idx in enumerate(random_idx_list): \n",
    "    print(f\"\\nExample {idx+1}\")\n",
    "    for key in ['text', 'attributes']: \n",
    "        print(f\"{key:12s}: {val_dataset[key][idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Upon looking at the examples, the output structured functional representation consists of a single function with attributes and attribute values. \n",
    "- According to the [VIGGO's dataset](https://huggingface.co/datasets/GEM/viggo) documentation, the function must be one of the following:\n",
    "    ```\n",
    "    ['inform', 'request', 'give_opinion', 'confirm', 'verify_attribute', 'suggest', 'request_explanation', 'recommend', 'request_attribute']\n",
    "    ```\n",
    "- While the attributes must be one of these:\n",
    "  ```\n",
    "  ['name', 'release_year', 'esrb', 'genres', 'platforms', 'available_on_steam', 'has_linux_release', 'has_mac_release', 'specifier', 'rating', 'player_perspective', 'has_multiplayer', 'developer', 'exp_release_date']\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt for generating the functional representation\n",
    "\n",
    "- Since I want to evaluate `GPT-4o` performance on this task, I will first try to build a prompt that can be used to generate the desired correct structured data.\n",
    "- Fortunately, I can get a head start on the prompt by starting with the prompt template used in [Anyscale's blog](https://www.anyscale.com/blog/fine-tuning-llama-2-a-comprehensive-case-study-for-tailoring-models-to-unique-applications) for the VIGGO dataset.\n",
    "- The prompt template is a few-shot prompt with examples from each function category to assist the model in understanding the intended output response representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Given a target sentence construct the underlying meaning representation of the input sentence as a single function with attributes and attribute values. \n",
    "This function should describe the target string accurately and the function must be one of the following ['inform', 'request', 'give_opinion', 'confirm', 'verify_attribute', 'suggest', 'request_explanation', 'recommend', 'request_attribute'].\n",
    "\n",
    "The attributes must be one of the following: ['name', 'exp_release_date', 'release_year', 'developer', 'esrb', 'rating', 'genres', 'player_perspective', 'has_multiplayer', 'platforms', 'available_on_steam', 'has_linux_release', 'has_mac_release', 'specifier']. The order your list the attributes within the function must follow the order listed above. For example the 'name' attribute must always come before the 'exp_release_date' attribute, and so forth.\n",
    "\n",
    "For each attribute, fill in the corresponding value of the attribute within brackets. A couple of examples are below. Note: you are to output the string after \"Output: \". Do not include \"Output: \" in your answer.\n",
    "\n",
    "Example 1)\n",
    "Sentence: Dirt: Showdown from 2012 is a sport racing game for the PlayStation, Xbox, PC rated E 10+ (for Everyone 10 and Older). It's not available on Steam, Linux, or Mac.\n",
    "Output: inform(name[Dirt: Showdown], release_year[2012], esrb[E 10+ (for Everyone 10 and Older)], genres[driving/racing, sport], platforms[PlayStation, Xbox, PC], available_on_steam[no], has_linux_release[no], has_mac_release[no])\n",
    "\n",
    "Example 2) \n",
    "Sentence: Were there even any terrible games in 2014?\n",
    "Output: request(release_year[2014], specifier[terrible])\n",
    "\n",
    "Example 3)\n",
    "Sentence: Adventure games that combine platforming and puzzles  can be frustrating to play, but the side view perspective is perfect for them. That's why I enjoyed playing Little Nightmares.\n",
    "Output: give_opinion(name[Little Nightmares], rating[good], genres[adventure, platformer, puzzle], player_perspective[side view])\n",
    "\n",
    "Example 4)\n",
    "Sentence: Since we're on the subject of games developed by Telltale Games, I'm wondering, have you played The Wolf Among Us?\n",
    "Output: recommend(name[The Wolf Among Us], developer[Telltale Games])\n",
    "\n",
    "Example 5) \n",
    "Sentence: Layers of Fear, the indie first person point-and-click adventure game?\n",
    "Output: confirm(name[Layers of Fear], genres[adventure, indie, point-and-click], player_perspective[first person])\t\n",
    "\n",
    "Example 6) \n",
    "Sentence: I bet you like it when you can play games on Steam, like Worms: Reloaded, right?\t\n",
    "Output: suggest(name[Worms: Reloaded], available_on_steam[yes])\n",
    "\n",
    "Example 7)\n",
    "Sentence: I recall you saying that you really enjoyed The Legend of Zelda: Ocarina of Time. Are you typically a big fan of games on Nintendo rated E (for Everyone)?\t\n",
    "Output: verify_attribute(name[The Legend of Zelda: Ocarina of Time], esrb[E (for Everyone)], rating[excellent], platforms[Nintendo])\n",
    "\n",
    "Example 8)\n",
    "Sentence: So what is it about the games that were released in 2005 that you find so excellent?\t\n",
    "Output: request_explanation(release_year[2005], rating[excellent])\n",
    "\n",
    "Example 9)\n",
    "Sentence: Do you think Mac is a better gaming platform than others?\n",
    "Output: request_attribute(has_mac_release[])\n",
    "\n",
    "Give the output for the following sentence:\n",
    "{input}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the prompt on random examples\n",
    "- I can evaluate the prompt on above random examples to get a sense of how well the model performs with the above prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"gpt-4o-2024-05-13\"\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary to store the responses\n",
    "gpt_responses_dict = {}\n",
    "\n",
    "# loop through the random indices and get the responses\n",
    "for i in random_idx_list:\n",
    "    \n",
    "    text = val_dataset[i][\"text\"]\n",
    "    ground_truth = val_dataset[i][\"attributes\"]\n",
    "    \n",
    "    prompt = PROMPT_TEMPLATE.format(input=text)\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=256,\n",
    "    )\n",
    "    output = response.choices[0].message.content\n",
    "    \n",
    "    gpt_responses_dict[i] = {\"text\": text, \"ground_truth\": ground_truth, \"output\": output}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth: inform(name[FIFA 12], release_year[2011], esrb[E (for Everyone)], rating[average], genres[simulation, sport])\n",
      "GPT Response: inform(name[FIFA 12], release_year[2011], esrb[E (for Everyone)], rating[average], genres[sports, simulation])\n",
      "\n",
      "\n",
      "Ground Truth: request(player_perspective[side view], specifier[easy])\n",
      "GPT Response: Output: request(genres[side view], rating[top], specifier[easy])\n",
      "\n",
      "\n",
      "Ground Truth: recommend(name[Need for Speed: The Run], platforms[Xbox])\n",
      "GPT Response: confirm(name[Need for Speed: The Run], platforms[Xbox])\n",
      "\n",
      "\n",
      "Ground Truth: verify_attribute(name[Assassin's Creed II], rating[good], genres[action-adventure, platformer], platforms[PlayStation])\n",
      "GPT Response: verify_attribute(name[Assassin's Creed II], rating[good], genres[action-adventure, platformer], platforms[PlayStation])\n",
      "\n",
      "\n",
      "Ground Truth: give_opinion(name[BioShock], developer[2K Boston], esrb[M (for Mature)], rating[good])\n",
      "GPT Response: Output: give_opinion(name[BioShock], developer[2K Boston], rating[good], esrb[M (Mature)])\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k,v in gpt_responses_dict.items():\n",
    "    print(f\"Ground Truth: {v['ground_truth']}\\nGPT Response: {v['output']}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improve the prompt template\n",
    "- Upon reviewing the above responses, it appears that the model is able to generate somewhat relevant structural representations for the given input sentences.\n",
    "- However, the responses are not always accurate, particularly regarding the attributes, their corresponding values, and their order.\n",
    "- Additionally, for some examples, the model's response starts with 'Output:', which is not ideal.\n",
    "- This issue might be due to the `Note: you are to output the string after \"Output: \"...` part being included too early in the prompt. A quick fix for this would be to move this part to the end of the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Given a target sentence construct the underlying meaning representation of the input sentence as a single function with attributes and attribute values. \n",
    "This function should describe the target string accurately and the function must be one of the following ['inform', 'request', 'give_opinion', 'confirm', 'verify_attribute', 'suggest', 'request_explanation', 'recommend', 'request_attribute'].\n",
    "\n",
    "The attributes must be one of the following: ['name', 'exp_release_date', 'release_year', 'developer', 'esrb', 'rating', 'genres', 'player_perspective', 'has_multiplayer', 'platforms', 'available_on_steam', 'has_linux_release', 'has_mac_release', 'specifier']. The order your list the attributes within the function must follow the order listed above. For example the 'name' attribute must always come before the 'exp_release_date' attribute, and so forth.\n",
    "\n",
    "For each attribute, fill in the corresponding value of the attribute within brackets. A couple of examples are below.\n",
    "\n",
    "Example 1)\n",
    "Sentence: Dirt: Showdown from 2012 is a sport racing game for the PlayStation, Xbox, PC rated E 10+ (for Everyone 10 and Older). It's not available on Steam, Linux, or Mac.\n",
    "Output: inform(name[Dirt: Showdown], release_year[2012], esrb[E 10+ (for Everyone 10 and Older)], genres[driving/racing, sport], platforms[PlayStation, Xbox, PC], available_on_steam[no], has_linux_release[no], has_mac_release[no])\n",
    "\n",
    "Example 2) \n",
    "Sentence: Were there even any terrible games in 2014?\n",
    "Output: request(release_year[2014], specifier[terrible])\n",
    "\n",
    "Example 3)\n",
    "Sentence: Adventure games that combine platforming and puzzles  can be frustrating to play, but the side view perspective is perfect for them. That's why I enjoyed playing Little Nightmares.\n",
    "Output: give_opinion(name[Little Nightmares], rating[good], genres[adventure, platformer, puzzle], player_perspective[side view])\n",
    "\n",
    "Example 4)\n",
    "Sentence: Since we're on the subject of games developed by Telltale Games, I'm wondering, have you played The Wolf Among Us?\n",
    "Output: recommend(name[The Wolf Among Us], developer[Telltale Games])\n",
    "\n",
    "Example 5) \n",
    "Sentence: Layers of Fear, the indie first person point-and-click adventure game?\n",
    "Output: confirm(name[Layers of Fear], genres[adventure, indie, point-and-click], player_perspective[first person])\t\n",
    "\n",
    "Example 6) \n",
    "Sentence: I bet you like it when you can play games on Steam, like Worms: Reloaded, right?\t\n",
    "Output: suggest(name[Worms: Reloaded], available_on_steam[yes])\n",
    "\n",
    "Example 7)\n",
    "Sentence: I recall you saying that you really enjoyed The Legend of Zelda: Ocarina of Time. Are you typically a big fan of games on Nintendo rated E (for Everyone)?\t\n",
    "Output: verify_attribute(name[The Legend of Zelda: Ocarina of Time], esrb[E (for Everyone)], rating[excellent], platforms[Nintendo])\n",
    "\n",
    "Example 8)\n",
    "Sentence: So what is it about the games that were released in 2005 that you find so excellent?\t\n",
    "Output: request_explanation(release_year[2005], rating[excellent])\n",
    "\n",
    "Example 9)\n",
    "Sentence: Do you think Mac is a better gaming platform than others?\n",
    "Output: request_attribute(has_mac_release[])\n",
    "\n",
    "Note: you are to output the string after \"Output: \". Do not include \"Output: \" in your answer.\n",
    "\n",
    "Give the output for the following sentence:\n",
    "{input}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary to store the responses\n",
    "gpt_responses_dict_update1 = {}\n",
    "\n",
    "# loop through the random indices and get the responses\n",
    "for i in random_idx_list:\n",
    "    \n",
    "    text = val_dataset[i][\"text\"]\n",
    "    ground_truth = val_dataset[i][\"attributes\"]\n",
    "    \n",
    "    prompt = PROMPT_TEMPLATE.format(input=text)\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=256,\n",
    "    )\n",
    "    output = response.choices[0].message.content\n",
    "    \n",
    "    gpt_responses_dict_update1[i] = {\"text\": text, \"ground_truth\": ground_truth, \"output\": output}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth: inform(name[FIFA 12], release_year[2011], esrb[E (for Everyone)], rating[average], genres[simulation, sport])\n",
      "GPT Response: inform(name[FIFA 12], release_year[2011], esrb[E (for Everyone)], rating[average], genres[sports simulation])\n",
      "\n",
      "\n",
      "Ground Truth: request(player_perspective[side view], specifier[easy])\n",
      "GPT Response: request_attribute(genres[easy], player_perspective[side view], rating[tops them all])\n",
      "\n",
      "\n",
      "Ground Truth: recommend(name[Need for Speed: The Run], platforms[Xbox])\n",
      "GPT Response: confirm(name[Need for Speed: The Run], platforms[Xbox])\n",
      "\n",
      "\n",
      "Ground Truth: verify_attribute(name[Assassin's Creed II], rating[good], genres[action-adventure, platformer], platforms[PlayStation])\n",
      "GPT Response: verify_attribute(name[Assassin's Creed II], rating[good], genres[action-adventure, platformer], platforms[PlayStation])\n",
      "\n",
      "\n",
      "Ground Truth: give_opinion(name[BioShock], developer[2K Boston], esrb[M (for Mature)], rating[good])\n",
      "GPT Response: give_opinion(name[BioShock], developer[2K Boston], rating[good], esrb[M])\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k,v in gpt_responses_dict_update1.items():\n",
    "    print(f\"Ground Truth: {v['ground_truth']}\\nGPT Response: {v['output']}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The responses are much better now, with no 'Output:' in the response. Although the responses are still not perfect, the prompt template can be used for evaluation purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation criteria for the task\n",
    "\n",
    "- Next, I need some evaluation criteria to assess the model's performance. I can evaluate the model's performance based on the following binary criteria:\n",
    "  1. **Function Name Match**: The function name must match the ground truth function name.\n",
    "  2. **Function and Attributes Match**: The generated function name and attributes must match the ground truth function attributes. However, the order of the attributes does not matter.\n",
    "  3. **Function, Attributes, and Values Match**: The generated function name, attributes, and values must match the ground truth function attributes and values. The order of the attributes and values does not matter.\n",
    "  4. **Exact Match**: The generated function must exactly match the ground truth function.\n",
    "\n",
    "- The above criteria are in order of increasing strictness. The first criterion is the least strict, and the last criterion is the most strict. These criteria will help me evaluate the model's performance.\n",
    "\n",
    "- **Although not ideal, I can use the `GPT-4` model to evaluate its own performance on the given task.**\n",
    "- I can write a prompt that asks the model to compare the generated function with the ground truth function and provide a score based on the above evaluation criteria. For example, I can use the following prompt to evaluate the model's performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE_SCORE = \"\"\"I will provide you with two functional representations strings. One will be the ground truth representation (ground_truth) and the other will be a generated representation (generated). You need to compare the generated representation with the ground truth representation and provide the following similarity match in true or false:\n",
    "1) function_match: The function name is the same but the attributes and values can be different.\n",
    "2) function_attributes_match: The function and the attributes are the same but the values can be different.\n",
    "3) function_attributes_values_match: The generated representation has same function and attributes and corresponding values without the same attributes and values order.\n",
    "4) exact_match: The generated representation is exactly the same as the ground truth representation.\n",
    "    \n",
    "A typical functional representation is of this form: function(attribute1[values], attribute2[values], attribute3[values], ...). \n",
    "\n",
    "Please provide the output in JSON format.\n",
    "\n",
    "Given the following two functional representation, provide the similarity scores for the following:\n",
    "\n",
    "ground_truth: {ground_truth}\n",
    "\n",
    "generated: {generated}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the evaluation score on some generated response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n",
      "Ground Truth: inform(name[FIFA 12], release_year[2011], esrb[E (for Everyone)], rating[average], genres[simulation, sport])\n",
      "GPT Response: inform(name[FIFA 12], release_year[2011], esrb[E (for Everyone)], rating[average], genres[sports simulation])\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"function_match\": true,\n",
      "  \"function_attributes_match\": true,\n",
      "  \"function_attributes_values_match\": false,\n",
      "  \"exact_match\": false\n",
      "}\n",
      "``` \n",
      "\n",
      "\n",
      "568\n",
      "Ground Truth: request(player_perspective[side view], specifier[easy])\n",
      "GPT Response: request_attribute(genres[easy], player_perspective[side view], rating[tops them all])\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"function_match\": false,\n",
      "  \"function_attributes_match\": false,\n",
      "  \"function_attributes_values_match\": false,\n",
      "  \"exact_match\": false\n",
      "}\n",
      "``` \n",
      "\n",
      "\n",
      "224\n",
      "Ground Truth: recommend(name[Need for Speed: The Run], platforms[Xbox])\n",
      "GPT Response: confirm(name[Need for Speed: The Run], platforms[Xbox])\n",
      "\n",
      "```json\n",
      "{\n",
      "    \"function_match\": false,\n",
      "    \"function_attributes_match\": false,\n",
      "    \"function_attributes_values_match\": false,\n",
      "    \"exact_match\": false\n",
      "}\n",
      "``` \n",
      "\n",
      "\n",
      "256\n",
      "Ground Truth: verify_attribute(name[Assassin's Creed II], rating[good], genres[action-adventure, platformer], platforms[PlayStation])\n",
      "GPT Response: verify_attribute(name[Assassin's Creed II], rating[good], genres[action-adventure, platformer], platforms[PlayStation])\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"function_match\": true,\n",
      "  \"function_attributes_match\": true,\n",
      "  \"function_attributes_values_match\": true,\n",
      "  \"exact_match\": true\n",
      "}\n",
      "``` \n",
      "\n",
      "\n",
      "394\n",
      "Ground Truth: give_opinion(name[BioShock], developer[2K Boston], esrb[M (for Mature)], rating[good])\n",
      "GPT Response: give_opinion(name[BioShock], developer[2K Boston], rating[good], esrb[M])\n",
      "\n",
      "```json\n",
      "{\n",
      "    \"function_match\": true,\n",
      "    \"function_attributes_match\": true,\n",
      "    \"function_attributes_values_match\": true,\n",
      "    \"exact_match\": false\n",
      "}\n",
      "``` \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the responses for the evaluation criteria\n",
    "for k,v in gpt_responses_dict_update1.items():\n",
    "    print(k)\n",
    "    print(f\"Ground Truth: {v['ground_truth']}\\nGPT Response: {v['output']}\\n\")\n",
    "        \n",
    "    prompt = PROMPT_TEMPLATE_SCORE.format(ground_truth=v['ground_truth'], generated=v['output'])\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=256,\n",
    "    )\n",
    "    output = response.choices[0].message.content\n",
    "    print(output,\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improve the evaluation task using `Instructor`\n",
    "\n",
    "- Looking at the output, I can see that the model is able to evaluate its own performance based on the provided evaluation criteria. \n",
    "- However, I need the evaluation scores in a more structured format to analyze the model's performance effectively. \n",
    "- A better way to achieve this is by using a package like [instructor](https://python.useinstructor.com/) to obtain the evaluation scores in a structured format. \n",
    "- I can use the following code and a modified score prompt template to get the evaluation scores in a structured format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pydantic model for the evaluation function\n",
    "\n",
    "class EvaluateFunctionRepresentation(BaseModel):\n",
    "    function_match: bool = Field(description=\"The function name is the same but the attributes and values can be different.\")\n",
    "    function_attribute_match: bool = Field(description=\"The function and the attributes are the same but the values can be different.\")\n",
    "    function_attributes_values_match: bool = Field(description=\"The generated representation has same function and attributes and corresponding values without the same attributes and values order.\")\n",
    "    exact_match: bool = Field(description=\"The generated representation is exactly the same as the ground truth representation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE_SCORE_EVAL = \"\"\"I will provide you with two functional representations strings. One will be the ground truth representation (ground_truth) and the other will be a generated representation (generated). You need to compare the generated representation with the ground truth representation and provide the following similarity match in true or false:\n",
    "1) function_match\n",
    "2) function_attributes_match\n",
    "3) function_attributes_values_match\n",
    "4) exact_match\n",
    "\n",
    "A typical functional representation is of this form: function(attribute1[values], attribute2[values], attribute3[values], ...). \n",
    "\n",
    "Given the following two functional representation, provide the similarity scores for the following:\n",
    "\n",
    "ground_truth: {ground_truth}\n",
    "\n",
    "generated: {generated}\n",
    "\n",
    "Let's think step by step.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_eval = instructor.patch(OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\")), mode=instructor.Mode.JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 120\n",
      "Ground Truth: inform(name[FIFA 12], release_year[2011], esrb[E (for Everyone)], rating[average], genres[simulation, sport])\n",
      "GPT Response: inform(name[FIFA 12], release_year[2011], esrb[E (for Everyone)], rating[average], genres[sports simulation])\n",
      "\n",
      "function_match                          : True\n",
      "function_attribute_match                : True\n",
      "function_attributes_values_match        : False\n",
      "exact_match                             : False\n",
      "\n",
      "\n",
      "\n",
      "Index: 568\n",
      "Ground Truth: request(player_perspective[side view], specifier[easy])\n",
      "GPT Response: request_attribute(genres[easy], player_perspective[side view], rating[tops them all])\n",
      "\n",
      "function_match                          : False\n",
      "function_attribute_match                : False\n",
      "function_attributes_values_match        : False\n",
      "exact_match                             : False\n",
      "\n",
      "\n",
      "\n",
      "Index: 224\n",
      "Ground Truth: recommend(name[Need for Speed: The Run], platforms[Xbox])\n",
      "GPT Response: confirm(name[Need for Speed: The Run], platforms[Xbox])\n",
      "\n",
      "function_match                          : False\n",
      "function_attribute_match                : False\n",
      "function_attributes_values_match        : False\n",
      "exact_match                             : False\n",
      "\n",
      "\n",
      "\n",
      "Index: 256\n",
      "Ground Truth: verify_attribute(name[Assassin's Creed II], rating[good], genres[action-adventure, platformer], platforms[PlayStation])\n",
      "GPT Response: verify_attribute(name[Assassin's Creed II], rating[good], genres[action-adventure, platformer], platforms[PlayStation])\n",
      "\n",
      "function_match                          : True\n",
      "function_attribute_match                : True\n",
      "function_attributes_values_match        : True\n",
      "exact_match                             : True\n",
      "\n",
      "\n",
      "\n",
      "Index: 394\n",
      "Ground Truth: give_opinion(name[BioShock], developer[2K Boston], esrb[M (for Mature)], rating[good])\n",
      "GPT Response: give_opinion(name[BioShock], developer[2K Boston], rating[good], esrb[M])\n",
      "\n",
      "function_match                          : True\n",
      "function_attribute_match                : True\n",
      "function_attributes_values_match        : False\n",
      "exact_match                             : False\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k,v in gpt_responses_dict_update1.items():\n",
    "    print(f\"Index: {k}\\nGround Truth: {v['ground_truth']}\\nGPT Response: {v['output']}\\n\")\n",
    "    \n",
    "    prompt = PROMPT_TEMPLATE_SCORE_EVAL.format(ground_truth=v['ground_truth'], generated=v['output'])\n",
    "\n",
    "    response = client_eval.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=256,\n",
    "        response_model=EvaluateFunctionRepresentation,\n",
    "        max_retries=2,\n",
    "    )\n",
    "    \n",
    "    for k,v in response.model_dump().items(): print(f\"{k:40s}: {v}\")\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now, the evaluation scores are in a structured format that can be easily analyzed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the responses for the full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example 0 of 714\n",
      "Processing example 100 of 714\n",
      "Processing example 200 of 714\n",
      "Processing example 300 of 714\n",
      "Processing example 400 of 714\n",
      "Processing example 500 of 714\n",
      "Processing example 600 of 714\n",
      "Processing example 700 of 714\n",
      "CPU times: user 5.86 s, sys: 262 ms, total: 6.12 s\n",
      "Wall time: 13min 22s\n"
     ]
    }
   ],
   "source": [
    "gpt_responses_dict = {}\n",
    "for i in range(len(val_dataset)):\n",
    "    if i % 100 == 0: print(f\"Processing example {i} of {len(val_dataset)}\")\n",
    "    text = val_dataset[i][\"text\"]\n",
    "    ground_truth = val_dataset[i][\"attributes\"]\n",
    "    \n",
    "    prompt = PROMPT_TEMPLATE.format(input=text)\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=256,\n",
    "    )\n",
    "    output = response.choices[0].message.content\n",
    "    \n",
    "    gpt_responses_dict[i] = {\"text\": text, \"ground_truth\": ground_truth, \"output\": output}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> took around 13 mins 22s to run 714 examples (approx 1.1s per example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the responses to a file for future reference\n",
    "with open(\"gpt_responses_viggo_eval.json\", \"w\") as f:\n",
    "    json.dump(gpt_responses_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the evaluation scores for the full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example 0 of 714\n",
      "Processing example 100 of 714\n",
      "Processing example 200 of 714\n",
      "Processing example 300 of 714\n",
      "Processing example 400 of 714\n",
      "Processing example 500 of 714\n",
      "Processing example 600 of 714\n",
      "Processing example 700 of 714\n"
     ]
    }
   ],
   "source": [
    "scores_dict = {}\n",
    "for i, (k,v) in enumerate(gpt_responses_dict.items()):\n",
    "    if i%100 ==0: print(f\"Processing example {i} of {len(gpt_responses_dict)}\")\n",
    "    prompt = PROMPT_TEMPLATE_SCORE_EVAL.format(ground_truth=v['ground_truth'], generated=v['output'])\n",
    "\n",
    "    response = client_eval.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=256,\n",
    "        response_model=EvaluateFunctionRepresentation,\n",
    "        max_retries=2,\n",
    "    )\n",
    "    \n",
    "    scores_dict[k] = response.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the average scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function_match                          : 0.80\n",
      "function_attribute_match                : 0.74\n",
      "function_attributes_values_match        : 0.49\n",
      "exact_match                             : 0.30\n"
     ]
    }
   ],
   "source": [
    "avg_scores = {k: sum([v[k] for v in scores_dict.values()])/len(scores_dict) for k in scores_dict[0].keys()}\n",
    "for k,v in avg_scores.items(): print(f\"{k:40s}: {v:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the average scores for the evaluation criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# make a bar plot of the scores \n",
    "scores_list = [round(avg_scores[k],2) for k in avg_scores.keys()]\n",
    "scores_labels = [k for k in avg_scores.keys()]\n",
    "\n",
    "# make the bar plot\n",
    "ax = sns.barplot(x=scores_list, y=scores_labels, hue=scores_labels, palette=\"viridis\")\n",
    "\n",
    "# set the title\n",
    "plt.title(\"GPT-4o Evaluation Scores\", fontsize=14, fontweight=\"bold\", fontname=\"Arial\")\n",
    "\n",
    "# set x and y labels\n",
    "plt.xlabel(\"Scores\", fontsize=11, fontname=\"Arial\", fontweight=\"bold\")\n",
    "plt.ylabel(\"\",fontsize=11, fontname=\"Arial\", fontweight=\"bold\")\n",
    "\n",
    "# set x and y ticks\n",
    "plt.yticks(fontsize=11, fontname=\"Arial\")\n",
    "plt.xticks(fontsize=11, fontname=\"Arial\", rotation=45)\n",
    "\n",
    "# set x-axis limits\n",
    "plt.xlim(0, 0.9)\n",
    "\n",
    "# score values on the bars\n",
    "for i in range(len(scores_list)):\n",
    "    plt.text(scores_list[i], i, scores_list[i], va='center', fontsize=12, fontname=\"Arial\", fontweight=\"bold\")\n",
    "\n",
    "# save the plot\n",
    "plt.savefig(\"gpt-4o-evaluation-scores.png\", dpi=300, bbox_inches=\"tight\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the evaluation scores, avg. scores to files for future reference.\n",
    "with open(\"gpt-4o-evaluation-scores.json\", \"w\") as f:\n",
    "    json.dump(scores_dict, f)\n",
    "\n",
    "with open(\"gpt-4o-avg-evaluation-scores.json\", \"w\") as f:\n",
    "    json.dump(avg_scores, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The task of generating functional representations from natural language sentences is challenging. The best the model can do is to provide the correct function name `80%` of the time whereas the exact match is only for 30% of the examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Alex Strict van Linchoten blog: [Evaluating the Baseline Performance of GPT-4-Turbo for Structured Data Extraction](https://mlops.systems/posts/2024-06-03-isafpr-evaluating-baseline.html)\n",
    "- Viggo's dataset: [HuggingFace Link](https://huggingface.co/datasets/GEM/viggo)\n",
    "- Anyscale's blog on fine-tuning Llama models for domain applications: [Fine-Tuning Llama-2: A Comprehensive Case Study for Tailoring Models to Unique Applications](https://www.anyscale.com/blog/fine-tuning-llama-2-a-comprehensive-case-study-for-tailoring-models-to-unique-applications)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
